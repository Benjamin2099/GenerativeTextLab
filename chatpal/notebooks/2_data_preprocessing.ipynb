{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 2_data_preprocessing.ipynb\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken & Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Falls du NLTK/Spacy/Hugging Face Tokenizer nutzen möchtest\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "RAW_DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Beispielhafte Datei: \"chatlogs_raw.csv\"\n",
    "# (Spalten user_message, bot_response, optional conversation_id)\n",
    "raw_file_path = os.path.join(RAW_DATA_DIR, \"chatlogs_raw.csv\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 2: Rohdaten laden\n",
    "# ===========================================\n",
    "df = pd.read_csv(raw_file_path)\n",
    "\n",
    "print(f\"Anzahl Rohdialoge: {len(df)}\")\n",
    "display(df.head())\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 3: Datensäuberung\n",
    "# ===========================================\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # HTML-Tags entfernen\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    # Mehrfache Leerzeichen\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Optional: Lowercase\n",
    "    text = text.strip().lower()\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_user\"] = df[\"user_message\"].apply(clean_text)\n",
    "df[\"cleaned_bot\"]  = df[\"bot_response\"].apply(clean_text)\n",
    "\n",
    "# Beispiel: Entferne Zeilen, die sehr kurz sind\n",
    "df = df[df[\"cleaned_user\"].str.len() > 1]\n",
    "df = df[df[\"cleaned_bot\"].str.len() > 1]\n",
    "\n",
    "print(\"\\nBeispiel bereinigter Datensätze:\")\n",
    "display(df[[\"cleaned_user\", \"cleaned_bot\"]].head())\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 4: Tokenisierung\n",
    "# ===========================================\n",
    "def tokenize_text(text):\n",
    "    # Simpler Word-Tokenizer (NLTK)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df[\"user_tokens\"] = df[\"cleaned_user\"].apply(tokenize_text)\n",
    "df[\"bot_tokens\"]  = df[\"cleaned_bot\"].apply(tokenize_text)\n",
    "\n",
    "print(\"\\nBeispiel für tokenisierte Sätze:\")\n",
    "display(df[[\"user_tokens\", \"bot_tokens\"]].head())\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 5: Vokabular aufbauen\n",
    "# ===========================================\n",
    "# Wir fassen alle Tokens aus User und Bot zusammen\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for uts, bts in zip(df[\"user_tokens\"], df[\"bot_tokens\"]):\n",
    "    all_tokens.extend(uts)\n",
    "    all_tokens.extend(bts)\n",
    "\n",
    "token_freqs = Counter(all_tokens)\n",
    "print(f\"Anzahl eindeutiger Tokens: {len(token_freqs)}\")\n",
    "\n",
    "# Beispiel: Wir begrenzen auf die häufigsten 10.000 Tokens\n",
    "VOCAB_SIZE = 10000\n",
    "most_common_tokens = token_freqs.most_common(VOCAB_SIZE)\n",
    "\n",
    "# Sondertokens\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS_USER>\", \"<EOS_USER>\", \"<BOS_BOT>\", \"<EOS_BOT>\"]\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for st in special_tokens:\n",
    "    word2id[st] = idx\n",
    "    idx += 1\n",
    "\n",
    "for token, freq in most_common_tokens:\n",
    "    if token not in word2id:\n",
    "        word2id[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "print(f\"\\nVokabulargröße (inkl. Sondertokens): {len(word2id)}\")\n",
    "\n",
    "# Inverses Mapping\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 6: Tokens -> IDs\n",
    "# ===========================================\n",
    "def tokens_to_ids(token_list, word2id, unk_id=word2id[\"<UNK>\"]):\n",
    "    return [word2id[t] if t in word2id else unk_id for t in token_list]\n",
    "\n",
    "UNK_ID = word2id[\"<UNK>\"]\n",
    "\n",
    "df[\"user_ids\"] = df[\"user_tokens\"].apply(lambda ts: tokens_to_ids(ts, word2id, UNK_ID))\n",
    "df[\"bot_ids\"]  = df[\"bot_tokens\"].apply(lambda ts: tokens_to_ids(ts, word2id, UNK_ID))\n",
    "\n",
    "# Optional: Füge BOS/EOS-Token hinzu\n",
    "# z. B.: <BOS_USER> user_ids <EOS_USER>, <BOS_BOT> bot_ids <EOS_BOT>\n",
    "BOS_USER_ID = word2id[\"<BOS_USER>\"]\n",
    "EOS_USER_ID = word2id[\"<EOS_USER>\"]\n",
    "BOS_BOT_ID  = word2id[\"<BOS_BOT>\"]\n",
    "EOS_BOT_ID  = word2id[\"<EOS_BOT>\"]\n",
    "\n",
    "df[\"user_ids\"] = df[\"user_ids\"].apply(lambda x: [BOS_USER_ID] + x + [EOS_USER_ID])\n",
    "df[\"bot_ids\"]  = df[\"bot_ids\"].apply(lambda x: [BOS_BOT_ID]  + x + [EOS_BOT_ID])\n",
    "\n",
    "print(\"\\nBeispiel für user_ids und bot_ids mit BOS/EOS:\")\n",
    "display(df[[\"user_ids\", \"bot_ids\"]].head())\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 7: Split in Train/Val\n",
    "# ===========================================\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "print(f\"Train Samples: {len(train_df)}, Val Samples: {len(val_df)}\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 8: Speichern\n",
    "# ===========================================\n",
    "import json\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Vokabular\n",
    "vocab_path = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2id, f, ensure_ascii=False)\n",
    "\n",
    "# CSVs\n",
    "train_path = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_path   = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "\n",
    "print(\"\\nGespeicherte Dateien:\")\n",
    "print(\"- Vokabular:\", vocab_path)\n",
    "print(\"- Train CSV:\", train_path)\n",
    "print(\"- Val CSV:  \", val_path)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 9: Ausblick\n",
    "# ===========================================\n",
    "print(\"\"\"\n",
    "Die Daten liegen nun tokenisiert (user_ids, bot_ids) in train.csv/val.csv.\n",
    "Wir können sie in Notebook 3_training_demo.ipynb oder via src/train.py \n",
    "laden und unser ChatPal-Modell trainieren.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
