{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken & Setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Gerät:\", DEVICE)\n",
    "\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 2: Daten laden\n",
    "# ===========================================\n",
    "train_csv = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_csv   = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "vocab_json= os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "\n",
    "with open(vocab_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k,v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "print(f\"Train Samples: {len(train_df)} | Val Samples: {len(val_df)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "\n",
    "# Annahme: train_df/val_df haben Spalten user_ids, bot_ids (List of ints im Stringformat wie \"[2,45,78]\")\n",
    "display(train_df.head(3))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 3: Dataset & DataLoader\n",
    "# ===========================================\n",
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Erwartet user_ids und bot_ids in Spalten (z. B. 'user_ids', 'bot_ids').\n",
    "    Jede Zeile repräsentiert ein User->Bot-Paar.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user_col=\"user_ids\", bot_col=\"bot_ids\"):\n",
    "        self.df = df\n",
    "        self.user_col = user_col\n",
    "        self.bot_col = bot_col\n",
    "        \n",
    "        # Konvertiere die Strings (z. B. \"[2, 45, 78]\") in Python-Listen\n",
    "        self.df[self.user_col] = self.df[self.user_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.df[self.bot_col]  = self.df[self.bot_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        self.user_samples = self.df[self.user_col].tolist()\n",
    "        self.bot_samples  = self.df[self.bot_col].tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_ids = self.user_samples[idx]\n",
    "        bot_ids  = self.bot_samples[idx]\n",
    "        \n",
    "        # Input=User, Target=Bot => Training: \"Vorhersage\" Bot\n",
    "        user_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "        bot_tensor  = torch.tensor(bot_ids,  dtype=torch.long)\n",
    "        \n",
    "        return user_tensor, bot_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    \"\"\"\n",
    "    Collate-Funktion, um User/Bot-Sequenzen in einem Batch mit Pad auf gleiche Länge zu bringen.\n",
    "    \"\"\"\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    user_seqs, bot_seqs = zip(*batch)\n",
    "    \n",
    "    user_padded = pad_sequence(user_seqs, batch_first=True, padding_value=pad_id)\n",
    "    bot_padded  = pad_sequence(bot_seqs,  batch_first=True, padding_value=pad_id)\n",
    "    \n",
    "    return user_padded, bot_padded\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = ChatDataset(train_df)\n",
    "val_dataset   = ChatDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, pad_id=word2id.get(\"<PAD>\",0))\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, pad_id=word2id.get(\"<PAD>\",0))\n",
    ")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 4: Modell definieren (z. B. LSTM)\n",
    "# ===========================================\n",
    "class LSTMChatModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Einfaches LSTM-Modell für Chat:\n",
    "    Eingabe=User (tokens), Ausgabe=Bot-Sequence pro Zeitschritt.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, pad_idx=0, dropout=0.0):\n",
    "        super(LSTMChatModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.fc   = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, user_seq, hidden=None):\n",
    "        # user_seq: [Batch, UserSeqLen]\n",
    "        emb = self.embedding(user_seq)              # -> [Batch, UserSeqLen, embed_dim]\n",
    "        out, hidden = self.lstm(emb, hidden)        # -> [Batch, UserSeqLen, hidden_dim]\n",
    "        logits = self.fc(out)                       # -> [Batch, UserSeqLen, vocab_size]\n",
    "        return logits, hidden\n",
    "\n",
    "model = LSTMChatModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    pad_idx=word2id.get(\"<PAD>\",0),\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 5: Trainings-Setup\n",
    "# ===========================================\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2id.get(\"<PAD>\",0))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 3\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for user_batch, bot_batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        user_batch = user_batch.to(DEVICE)\n",
    "        bot_batch  = bot_batch.to(DEVICE)\n",
    "        \n",
    "        logits, _ = model(user_batch)\n",
    "        # logits: [Batch, UserSeqLen, vocab_size]\n",
    "        # Wir wollen, dass unser Modell die Bot-Sequenz Wort-für-Wort predicten kann.\n",
    "        # Einfachster Ansatz: Gleiche Länge wie user_seq -> suboptimal, \n",
    "        # aber als Demo ok. (Besser: Concat user_seq + shifted bot_seq)\n",
    "        \n",
    "        vocab_s = logits.size(-1)\n",
    "        logits = logits.reshape(-1, vocab_s)  # [Batch*UserSeqLen, vocab_size]\n",
    "        bot_flat = bot_batch.view(-1)        # [Batch*UserSeqLen]\n",
    "        \n",
    "        loss = criterion(logits, bot_flat)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user_batch, bot_batch in loader:\n",
    "            user_batch = user_batch.to(DEVICE)\n",
    "            bot_batch  = bot_batch.to(DEVICE)\n",
    "            \n",
    "            logits, _ = model(user_batch)\n",
    "            vocab_s = logits.size(-1)\n",
    "            logits = logits.view(-1, vocab_s)\n",
    "            bot_flat = bot_batch.view(-1)\n",
    "            loss = criterion(logits, bot_flat)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 6: Training starten\n",
    "# ===========================================\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\n=== Epoche {epoch}/{EPOCHS} ===\")\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_loss   = evaluate(model, val_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"chatpal_best_epoch{epoch}_valloss{val_loss:.4f}.pt\")\n",
    "        print(\"** Neues bestes Modell gespeichert\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 7: Kurzer Inferenz-Test (simple)\n",
    "# ===========================================\n",
    "def sample_token(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Greedy oder random-Sampling je nach dem, was du willst.\n",
    "    Hier: Greedy\n",
    "    \"\"\"\n",
    "    scaled_logits = logits / max(temperature, 1e-8)\n",
    "    return torch.argmax(scaled_logits).item()\n",
    "\n",
    "def respond(model, user_tokens, max_len=20):\n",
    "    \"\"\"\n",
    "    Einfache Generierung:\n",
    "    - Eingabe: user_tokens (Liste von IDs)\n",
    "    - Ausgabe: Bot-Vorhersage pro Zeitstep\n",
    "      (hier vereinfachtes unvollständiges Verfahren, \n",
    "       nur um Idee zu zeigen; \n",
    "       i. d. R. müsste man ein Chat-Decoder-Ansatz oder \n",
    "       Concat user+bot implementieren)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_seq = torch.tensor([user_tokens], dtype=torch.long).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(user_seq)  # [1, seq_len, vocab_size]\n",
    "        # Nimm die letzten Zeitsteps nacheinander, \n",
    "        # hier sehr vereinfacht:\n",
    "        # Besser wäre: user_seq -> <BOS_BOT> generieren\n",
    "        generated = []\n",
    "        # Greedy: Letztes Zeitstep hernehmen\n",
    "        for t in range(min(max_len, logits.shape[1])):\n",
    "            token_id = sample_token(logits[0, t, :], temperature=1.0)\n",
    "            generated.append(token_id)\n",
    "    return generated\n",
    "\n",
    "test_user_text = \"hello\"\n",
    "test_user_tokens = []\n",
    "for w in test_user_text.split():\n",
    "    test_user_tokens.append(word2id.get(w, word2id.get(\"<UNK>\",1)))\n",
    "# Füge BOS/EOS hinzu, falls nötig:\n",
    "# (z. B. <BOS_USER> + user_tokens + <EOS_USER>)\n",
    "BOS_USER_ID = word2id.get(\"<BOS_USER>\", 2)\n",
    "EOS_USER_ID = word2id.get(\"<EOS_USER>\", 3)\n",
    "test_user_tokens = [BOS_USER_ID] + test_user_tokens + [EOS_USER_ID]\n",
    "\n",
    "gen_bot_ids = respond(model, test_user_tokens, max_len=10)\n",
    "gen_bot_tokens = [id2word.get(i, \"<UNK>\") for i in gen_bot_ids]\n",
    "print(\"\\nBeispiel Inferenz: \")\n",
    "print(\"User:\", test_user_text)\n",
    "print(\"Bot :\", \" \".join(gen_bot_tokens))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 8: Fazit\n",
    "# ===========================================\n",
    "print(\"\"\"\n",
    "In diesem Notebook haben wir ein einfaches LSTM-Modell trainiert, \n",
    "das bei Eingabe von user_tokens ein Bot-Vorhersage-Logits liefert.\n",
    "Das Inferenzverfahren ist noch sehr vereinfacht; \n",
    "in einem richtigen Chat-Szenario würde man \n",
    "auch Bot-Token schrittweise decodieren \n",
    "und den gesamten Kontext (User + Bot Historie) beachten.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
