{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken & Setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Gerät:\", DEVICE)\n",
    "\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 2: Datensätze & Vokabular laden\n",
    "# ===========================================\n",
    "train_csv = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_csv   = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "test_csv  = os.path.join(PROCESSED_DATA_DIR, \"test.csv\")\n",
    "vocab_path= os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "test_df  = pd.read_csv(test_csv)\n",
    "\n",
    "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "print(f\"Train-Samples: {len(train_df)}, Val-Samples: {len(val_df)}, Test-Samples: {len(test_df)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 3: Dataset & DataLoader\n",
    "# ===========================================\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Erwartet eine CSV mit Spalte \"token_ids\", \n",
    "    in der z. B. '[2, 45, 7, 13]' (List of ints) steht.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, seq_col=\"token_ids\"):\n",
    "        self.samples = df[seq_col].apply(lambda x: eval(x) if isinstance(x, str) else x).tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.samples[idx]\n",
    "        # input_seq: tokens[:-1], target_seq: tokens[1:]\n",
    "        # optional: BOS=2, EOS=3\n",
    "        if len(tokens) < 2:\n",
    "            tokens = [2, 3]  # fallback <BOS>, <EOS>\n",
    "        inp = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        tgt = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return inp, tgt\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    \"\"\" Padding der Sequenzen im Batch. \"\"\"\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    inputs, targets = zip(*batch)\n",
    "    inp_padded = pad_sequence(inputs, batch_first=True, padding_value=pad_id)\n",
    "    tgt_padded = pad_sequence(targets, batch_first=True, padding_value=pad_id)\n",
    "    return inp_padded, tgt_padded\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TextDataset(train_df)\n",
    "val_dataset   = TextDataset(val_df)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Test-Dataset kannst du für abschließende Auswertungen behalten\n",
    "# oder später in Inferenz-Szenarien nutzen.\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 4: LSTM-Modell definieren\n",
    "# ===========================================\n",
    "class LSTMTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1, pad_idx=0):\n",
    "        super(LSTMTextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "# Modellinstanz\n",
    "model = LSTMTextModel(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=128, \n",
    "    hidden_dim=256, \n",
    "    num_layers=2, \n",
    "    pad_idx=word2id[\"<PAD>\"]  # In der Annahme, dass <PAD>=0\n",
    ")\n",
    "model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 5: Trainingssetup\n",
    "# ===========================================\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 3\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 6: Trainingsschleife\n",
    "# ===========================================\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inp, tgt in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        inp = inp.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        logits, _ = model(inp)\n",
    "        vocab_s = logits.size(-1)\n",
    "        logits = logits.view(-1, vocab_s)\n",
    "        tgt = tgt.view(-1)\n",
    "        \n",
    "        loss = criterion(logits, tgt)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in loader:\n",
    "            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n",
    "            logits, _ = model(inp)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            tgt = tgt.view(-1)\n",
    "            loss = criterion(logits, tgt)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"lstm_best_model_epoch{epoch+1}.pt\")\n",
    "        print(\"** Neues bestes Modell gespeichert.\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 7: Einfache Generierung (Greedy)\n",
    "# ===========================================\n",
    "def generate_text(model, prompt_tokens, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = prompt_tokens[:]\n",
    "    inp = torch.tensor([tokens], dtype=torch.long).to(DEVICE)\n",
    "    hidden = None\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        logits, hidden = model(inp, hidden)\n",
    "        # logits: [1, seq_len, vocab_size], wir nehmen das letzte Token\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        next_id = torch.argmax(last_token_logits).item()\n",
    "        \n",
    "        tokens.append(next_id)\n",
    "        inp = torch.tensor([tokens], dtype=torch.long).to(DEVICE)\n",
    "        \n",
    "        if next_id == word2id.get(\"<EOS>\", -1):\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "prompt_text = [\"<BOS>\"]\n",
    "prompt_ids  = [word2id.get(t, word2id.get(\"<UNK>\", 1)) for t in prompt_text]\n",
    "\n",
    "generated_ids = generate_text(model, prompt_ids, max_len=20)\n",
    "generated_tokens = [id2word.get(i, \"<UNK>\") for i in generated_ids]\n",
    "\n",
    "print(\"Generierter Text (Greedy):\")\n",
    "print(\" \".join(generated_tokens))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 8: Fazit & Ausblick\n",
    "# ===========================================\n",
    "print(\"\"\"\n",
    "In diesem Notebook wurde ein einfaches LSTM-Modell trainiert,\n",
    "um Geschichten fortzusetzen. Mit mehr Epochen und Daten kann die\n",
    "Textqualität gesteigert werden.\n",
    "\n",
    "Nächste Schritte:\n",
    "- Mehr Epochen & Hyperparameter-Tuning\n",
    "- Top-k oder Temperature-Sampling (anstatt Greedy)\n",
    "- Upgrade auf Transformer-Architekturen, z. B. GPT-2\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
