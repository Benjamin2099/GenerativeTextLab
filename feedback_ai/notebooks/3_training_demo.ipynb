{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# Training mit Feedback-basiertem Fine-Tuning (LSTM/Transformer)\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Importiere benötigte Bibliotheken und Setup\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Trainiere auf Gerät:\", DEVICE)\n",
    "\n",
    "# Definiere Pfade\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAIN_CSV = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "VAL_CSV = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "VOCAB_JSON = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "# Zelle 2: Lade Daten und Vokabular\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_val = pd.read_csv(VAL_CSV)\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "pad_id = word2id.get(\"<PAD>\", 0)\n",
    "print(f\"Train Samples: {len(df_train)} | Val Samples: {len(df_val)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "\n",
    "# Zelle 3: Definiere Dataset-Klasse inklusive Feedback-Label\n",
    "class FeedbackDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset für Feedback AI.\n",
    "    \n",
    "    Erwartet eine CSV-Datei mit folgenden Spalten:\n",
    "      - \"text_ids\": Tokenisierte Textsequenz (Liste als String, z. B. \"[2, 45, 78, 3]\")\n",
    "      - \"feedback_label\": Label, das das Nutzerfeedback repräsentiert (z.B. 1 für thumbs_up, 0 für thumbs_down)\n",
    "    \n",
    "    Bei jedem Zugriff liefert __getitem__ ein Tupel (text_tensor, text_tensor, feedback_label),\n",
    "    wobei hier für ein Seq2Seq-Training Input und Target identisch sein können, wenn der Text vervollständigt werden soll.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, text_col=\"token_ids\", label_col=\"feedback_label\"):\n",
    "        self.df = df.copy()\n",
    "        self.df[text_col] = self.df[text_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.df[label_col] = self.df[label_col].astype(int)\n",
    "        self.texts = self.df[text_col].tolist()\n",
    "        self.labels = self.df[label_col].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_ids = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Hier nutzen wir den gleichen Text als Input und Target (um Vervollständigungen zu trainieren)\n",
    "        text_tensor = torch.tensor(text_ids, dtype=torch.long)\n",
    "        return text_tensor, text_tensor, label  # (Input, Target, Feedback)\n",
    "\n",
    "def feedback_collate_fn(batch, pad_id=0):\n",
    "    texts_in, texts_out, labels = zip(*batch)\n",
    "    texts_in_padded = pad_sequence(texts_in, batch_first=True, padding_value=pad_id)\n",
    "    texts_out_padded = pad_sequence(texts_out, batch_first=True, padding_value=pad_id)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)  # Als float für Gewichtung\n",
    "    return texts_in_padded, texts_out_padded, labels_tensor\n",
    "\n",
    "def create_feedback_dataloader(csv_file, batch_size=32, shuffle=True, pad_id=0, text_col=\"token_ids\", label_col=\"feedback_label\"):\n",
    "    dataset = FeedbackDataset(pd.read_csv(csv_file), text_col=text_col, label_col=label_col)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: feedback_collate_fn(batch, pad_id=pad_id)\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "train_loader = create_feedback_dataloader(TRAIN_CSV, batch_size=32, shuffle=True, pad_id=pad_id)\n",
    "val_loader = create_feedback_dataloader(VAL_CSV, batch_size=32, shuffle=False, pad_id=pad_id)\n",
    "\n",
    "print(\"Beispielhafter Batch:\")\n",
    "batch_in, batch_out, batch_labels = next(iter(train_loader))\n",
    "print(\"Input-Shape:\", batch_in.shape, \"Labels-Shape:\", batch_labels.shape)\n",
    "\n",
    "# Zelle 4: Modell definieren (Seq2Seq-LSTM)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, trg, hidden):\n",
    "        embedded = self.embedding(trg)\n",
    "        outputs, hidden = self.lstm(embedded, hidden)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        input_dec = trg[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_dec, hidden)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(dim=2)\n",
    "            input_dec = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "decoder = Decoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "model = Seq2Seq(encoder, decoder, pad_id).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Zelle 5: Trainings-Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"none\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 5\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # Wir berechnen den Verlust pro Beispiel und multiplizieren ihn mit dem Feedback-Gewicht.\n",
    "    for src, trg, feedback in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        src, trg, feedback = src.to(device), trg.to(device), feedback.to(device)  # feedback: [Batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        # outputs: [Batch, trg_len, vocab_size]\n",
    "        output_dim = outputs.shape[-1]\n",
    "        outputs = outputs[:, 1:].reshape(-1, output_dim)  # Ignoriere <BOS>\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        # Berechne den Verlust pro Token (ohne Mittelung)\n",
    "        loss_per_token = criterion(outputs, trg)  # [Batch*trg_len]\n",
    "        # Reshape, um den Verlust pro Beispiel zu summieren:\n",
    "        loss_per_example = loss_per_token.view(src.size(0), -1).sum(dim=1)  # [Batch]\n",
    "        # Feedback-Gewichtung: Angenommen, positives Feedback (Label 1) erhält Gewicht 1, negatives (0) 0.5\n",
    "        weights = torch.where(feedback == 1, torch.tensor(1.0, device=device), torch.tensor(0.5, device=device))\n",
    "        weighted_loss = (loss_per_example * weights).mean()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += weighted_loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, _ in loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.mean().item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Zelle 6: Trainings- und Validierungsschleife\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoche {epoch}/{EPOCHS} ===\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, teacher_forcing_ratio=0.5)\n",
    "    val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = os.path.join(\"models\", f\"feedback_ai_epoch{epoch}_valloss{val_loss:.4f}.pt\")\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"** Neues bestes Modell gespeichert: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining abgeschlossen.\")\n",
    "\n",
    "# Zelle 7: Inferenz-Demo – Generierung eines Feedback-gesteuerten Textes\n",
    "def generate_feedback_text(model, input_ids, word2id, id2word, max_length=20, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    src_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    _, hidden = encoder(src_tensor)\n",
    "    bos_id = word2id.get(\"<BOS>\", 2)\n",
    "    input_dec = torch.tensor([[bos_id]], dtype=torch.long).to(device)\n",
    "    generated_ids = [bos_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = decoder(input_dec, hidden)\n",
    "            next_id = output.argmax(dim=2).item()\n",
    "            if next_id == word2id.get(\"<EOS>\", 3):\n",
    "                break\n",
    "            generated_ids.append(next_id)\n",
    "            input_dec = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    generated_text = \" \".join([id2word.get(i, \"<UNK>\") for i in generated_ids])\n",
    "    return generated_text\n",
    "\n",
    "# Beispiel-Inferenz:\n",
    "example_input = \"sehr geehrte damen und herren, ich möchte sie darüber informieren\"\n",
    "# Tokenisierung: einfach per split (in der Praxis dieselbe Tokenisierung wie in der Vorverarbeitung)\n",
    "example_tokens = example_input.strip().lower().split()\n",
    "input_ids = [word2id.get(token, word2id.get(\"<UNK>\", 1)) for token in example_tokens]\n",
    "\n",
    "completion = generate_feedback_text(model, input_ids, word2id, id2word, max_length=20, device=DEVICE)\n",
    "print(\"\\nBeispiel generierter Textvorschlag:\")\n",
    "print(\"Input:\", example_input)\n",
    "print(\"Vervollständigung:\", completion)\n",
    "\n",
    "# Zelle 8: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Fazit:\n",
    "- Das Modell wurde mit Feedback-basiertem Fine-Tuning trainiert.\n",
    "- Positive Feedback-Beispiele wurden höher gewichtet als negative.\n",
    "- Erste Inferenz-Demos zeigen, dass das Modell bevorzugt Texte generiert, die positives Feedback erhalten.\n",
    "Nächste Schritte:\n",
    "- Training auf einem größeren Datensatz.\n",
    "- Erweiterung auf Transformer-basierte Ansätze (z.B. GPT-2/T5) mit RLHF.\n",
    "- Analyse der Korrelation zwischen bestimmten Textmustern und dem Nutzerfeedback.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
