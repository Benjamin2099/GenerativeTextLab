{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 4_rlhf_experiment.ipynb\n",
    "# Experimentelles Reinforcement Learning mit menschlichem Feedback (RLHF)\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken und Setup\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Experiment läuft auf:\", DEVICE)\n",
    "\n",
    "# Für diesen Experimentellen RLHF-Demo-Ansatz verwenden wir das zuvor trainierte Seq2Seq-Modell.\n",
    "# Wir simulieren hier menschliches Feedback in Form eines Rewards:\n",
    "# - \"thumbs_up\" (Label 1) => Reward = +1.0\n",
    "# - \"thumbs_down\" (Label 0) => Reward = 0.0\n",
    "\n",
    "# Zelle 2: Dummy-Daten und Feedback-Simulation\n",
    "# Simuliere einen Batch von Beispielen:\n",
    "# Angenommen, unser Modell generiert zu jedem Beispiel eine Sequenz von Token-IDs.\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "vocab_size = 500  # Dummy-Vokabulargröße\n",
    "\n",
    "# Simuliere Eingaben (Input-Text) als zufällige Token-IDs\n",
    "dummy_inputs = torch.randint(0, vocab_size, (batch_size, seq_len)).to(DEVICE)\n",
    "\n",
    "# Simuliere Target-Sequenzen (wir nehmen hier gleiche Länge an)\n",
    "dummy_targets = torch.randint(0, vocab_size, (batch_size, seq_len)).to(DEVICE)\n",
    "\n",
    "# Simuliere Feedback-Labels: 1 für positives Feedback, 0 für negatives Feedback\n",
    "dummy_feedback = torch.tensor([1, 0, 1, 0], dtype=torch.float).to(DEVICE)  # Beispiel-Batch\n",
    "\n",
    "# Zelle 3: Dummy Seq2Seq-Modell (wie zuvor definiert)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, trg, hidden):\n",
    "        embedded = self.embedding(trg)\n",
    "        outputs, hidden = self.lstm(embedded, hidden)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        _, hidden = self.encoder(src)\n",
    "        input_dec = trg[:, 0].unsqueeze(1)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_dec, hidden)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(dim=2)\n",
    "            input_dec = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Dummy-Pad-ID (z.B. 0)\n",
    "pad_id = 0\n",
    "\n",
    "# Initialisiere das Dummy-Modell\n",
    "encoder_model = Encoder(vocab_size, embed_dim=64, hidden_dim=128, num_layers=1, pad_idx=pad_id)\n",
    "decoder_model = Decoder(vocab_size, embed_dim=64, hidden_dim=128, num_layers=1, pad_idx=pad_id)\n",
    "model = Seq2Seq(encoder_model, decoder_model, pad_id).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Zelle 4: Standard-Loss berechnen (ohne Feedback)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"none\")\n",
    "model.train()\n",
    "outputs = model(dummy_inputs, dummy_targets, teacher_forcing_ratio=0.5)\n",
    "output_dim = outputs.shape[-1]\n",
    "outputs_flat = outputs[:, 1:].reshape(-1, output_dim)\n",
    "trg_flat = dummy_targets[:, 1:].reshape(-1)\n",
    "loss_tokens = criterion(outputs_flat, trg_flat)\n",
    "loss_standard = loss_tokens.mean()\n",
    "print(\"Standard Loss (ohne Feedback):\", loss_standard.item())\n",
    "\n",
    "# Zelle 5: Feedback-basiertes Fine-Tuning mittels REINFORCE\n",
    "# Wir nutzen einen vereinfachten REINFORCE-Ansatz: \n",
    "# Für jedes Beispiel wird der Gesamtverlust (über die Sequenz) berechnet und\n",
    "# dann mit dem Feedback-Rewards multipliziert, bevor gemittelt wird.\n",
    "\n",
    "def compute_weighted_loss(outputs, trg, feedback, criterion, pad_id):\n",
    "    \"\"\"\n",
    "    Berechnet den gewichteten Verlust, wobei pro Beispiel der Verlust (Summe über Tokens)\n",
    "    mit einem Reward multipliziert wird. Positive Feedbacks erhalten höheren Reward.\n",
    "    \"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    output_dim = outputs.shape[-1]\n",
    "    # Ignoriere das erste Token (<BOS>)\n",
    "    outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "    trg = trg[:, 1:].reshape(-1)\n",
    "    loss_per_token = criterion(outputs, trg)  # [Batch*seq_len]\n",
    "    # Summe pro Beispiel\n",
    "    loss_per_example = loss_per_token.view(batch_size, -1).sum(dim=1)  # [Batch]\n",
    "    # Definiere Reward: positives Feedback (Label 1) => Reward 1.0, negatives (0) => 0.5\n",
    "    rewards = torch.where(feedback == 1, torch.tensor(1.0, device=DEVICE), torch.tensor(0.5, device=DEVICE))\n",
    "    weighted_loss = (loss_per_example * rewards).mean()\n",
    "    return weighted_loss\n",
    "\n",
    "# Simuliere einen Trainingsschritt mit Feedback\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = model(dummy_inputs, dummy_targets, teacher_forcing_ratio=0.5)\n",
    "weighted_loss = compute_weighted_loss(outputs, dummy_targets, dummy_feedback, criterion, pad_id)\n",
    "weighted_loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Feedback-gewichteter Loss:\", weighted_loss.item())\n",
    "\n",
    "# Zelle 6: Experimentelle Inferenz mit Feedback (ohne direkte RL-Update)\n",
    "def generate_text(model, input_ids, word2id, id2word, max_length=20, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    src_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    _, hidden = encoder_model(src_tensor)\n",
    "    bos_id = word2id.get(\"<BOS>\", 2)\n",
    "    input_dec = torch.tensor([[bos_id]], dtype=torch.long).to(device)\n",
    "    generated_ids = [bos_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = decoder_model(input_dec, hidden)\n",
    "            next_id = output.argmax(dim=2).item()\n",
    "            if next_id == word2id.get(\"<EOS>\", 3):\n",
    "                break\n",
    "            generated_ids.append(next_id)\n",
    "            input_dec = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    generated_text = \" \".join([id2word.get(i, \"<UNK>\") for i in generated_ids])\n",
    "    return generated_text\n",
    "\n",
    "# Beispiel-Inferenz: Simuliere einen Input\n",
    "dummy_input = dummy_inputs[0].tolist()  # Nutze einen Dummy-Input aus dem Batch\n",
    "generated = generate_text(model, dummy_input, { \"<BOS>\": 2, \"<EOS>\": 3, \"<UNK>\": 1 }, { 2: \"<BOS>\", 3: \"<EOS>\", 1: \"<UNK>\" }, max_length=20, device=DEVICE)\n",
    "print(\"\\nGenerierter Text (Feedback-RLHF-Demo):\")\n",
    "print(generated)\n",
    "\n",
    "# Zelle 7: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Fazit:\n",
    "- Dieser experimentelle Ansatz zeigt, wie man menschliches Feedback (als Reward) in das Training integrieren kann.\n",
    "- Durch die Gewichtung des Verlusts werden positive Beispiele stärker gewichtet, was das Modell dazu anregt, qualitativ bessere Texte zu generieren.\n",
    "- Dies ist ein vereinfachtes Beispiel für Reinforcement Learning from Human Feedback (RLHF).\n",
    "\n",
    "Nächste Schritte:\n",
    "- Teste diesen Ansatz mit echten Nutzerdaten und evaluiere die Verbesserungen.\n",
    "- Experimentiere mit alternativen Reward-Funktionen und erweitere den Ansatz auf Transformer-Modelle (z. B. GPT-2/T5) mit RLHF.\n",
    "- Integriere fortgeschrittene RL-Methoden (z. B. Policy-Gradient-Ansätze) für eine robustere Optimierung.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
