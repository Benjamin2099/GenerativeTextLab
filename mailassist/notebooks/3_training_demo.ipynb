{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# Training & Evaluierung der E-Mail-Vervollständigung\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken und Setup\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Trainiere auf Gerät:\", DEVICE)\n",
    "\n",
    "# Definiere Pfade zu den verarbeiteten Daten\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAIN_CSV = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "VAL_CSV = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "VOCAB_JSON = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "# Zelle 2: Daten und Vokabular laden\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_val = pd.read_csv(VAL_CSV)\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "pad_id = word2id.get(\"<PAD>\", 0)\n",
    "print(f\"Train Samples: {len(df_train)} | Val Samples: {len(df_val)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "display(df_train.head(3))\n",
    "\n",
    "# Zelle 3: Dataset-Klasse definieren\n",
    "class MailDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset für die E-Mail-Vervollständigung.\n",
    "    Erwartet, dass die CSV-Dateien die Spalten \"subject_ids\" und \"body_ids\" enthalten.\n",
    "    Hierbei werden die E-Mail-Inhalte als Input (z.B. unvollständiger Text) und die erwartete Vervollständigung als Target genutzt.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, input_col=\"body_ids\", target_col=\"body_ids\"):\n",
    "        # Bei MailAssist kann der Input z. B. der Anfang eines E-Mail-Bodys sein,\n",
    "        # und das Target die komplette E-Mail oder die fehlenden Teile.\n",
    "        self.df = df.copy()\n",
    "        self.df[input_col] = self.df[input_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.df[target_col] = self.df[target_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.inputs = self.df[input_col].tolist()\n",
    "        self.targets = self.df[target_col].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp_ids = self.inputs[idx]\n",
    "        tgt_ids = self.targets[idx]\n",
    "        inp_tensor = torch.tensor(inp_ids, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long)\n",
    "        return inp_tensor, tgt_tensor\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=pad_id)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=pad_id)\n",
    "    return inputs_padded, targets_padded\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MailDataset(df_train, input_col=\"body_ids\", target_col=\"body_ids\")\n",
    "val_dataset = MailDataset(df_val, input_col=\"body_ids\", target_col=\"body_ids\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id))\n",
    "\n",
    "print(\"Beispielhafter Batch:\")\n",
    "sample_inp, sample_tgt = next(iter(train_loader))\n",
    "print(\"Input-Shape:\", sample_inp.shape, \"Target-Shape:\", sample_tgt.shape)\n",
    "\n",
    "# Zelle 4: Modell definieren (Seq2Seq-LSTM für E-Mail-Vervollständigung)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # [Batch, src_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, trg, hidden):\n",
    "        embedded = self.embedding(trg)  # [Batch, trg_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded, hidden)\n",
    "        predictions = self.fc(outputs)  # [Batch, trg_len, vocab_size]\n",
    "        return predictions, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Der erste Decoder-Input ist das <BOS>-Token aus der Zielsequenz\n",
    "        input_dec = trg[:, 0].unsqueeze(1)  # [Batch, 1]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_dec, hidden)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(dim=2)  # [Batch, 1]\n",
    "            input_dec = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "decoder = Decoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "model = Seq2Seq(encoder, decoder, pad_id).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Zelle 5: Trainings-Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for src, trg in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        output_dim = outputs.shape[-1]\n",
    "        outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(outputs, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Zelle 6: Trainings- und Validierungsschleife\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoche {epoch}/{EPOCHS} ===\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, teacher_forcing_ratio=0.5)\n",
    "    val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = os.path.join(\"models\", f\"mailassist_epoch{epoch}_valloss{val_loss:.4f}.pt\")\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"** Neues bestes Modell gespeichert: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining abgeschlossen.\")\n",
    "\n",
    "# Zelle 7: Inferenz-Demo – E-Mail-Vervollständigung\n",
    "def generate_completion(model, input_ids, word2id, id2word, max_length=20, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generiert eine E-Mail-Vervollständigung basierend auf einem gegebenen Input.\n",
    "    \n",
    "    Args:\n",
    "        model: Das Seq2Seq-Modell.\n",
    "        input_ids: Liste von Token-IDs, die den Anfang der E-Mail repräsentieren.\n",
    "        word2id: Mapping von Token zu ID.\n",
    "        id2word: Inverses Mapping.\n",
    "        max_length: Maximale Anzahl an generierten Tokens (ohne das initiale <BOS>).\n",
    "        device: \"cuda\" oder \"cpu\".\n",
    "        \n",
    "    Returns:\n",
    "        Generierter Text als String.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Konvertiere den Input in einen Tensor\n",
    "    src_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    # Encoder: Verarbeite den Input\n",
    "    _, hidden = encoder(src_tensor)\n",
    "    \n",
    "    # Initialer Decoder-Input: <BOS>-Token der Zielsequenz\n",
    "    bos_id = word2id.get(\"<BOS>\", 2)\n",
    "    input_dec = torch.tensor([[bos_id]], dtype=torch.long).to(device)\n",
    "    generated_ids = [bos_id]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = decoder(input_dec, hidden)\n",
    "            next_id = output.argmax(dim=2).item()\n",
    "            if next_id == word2id.get(\"<EOS>\", 3):\n",
    "                break\n",
    "            generated_ids.append(next_id)\n",
    "            input_dec = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Konvertiere generierte IDs in Text\n",
    "    generated_text = \" \".join([id2word.get(i, \"<UNK>\") for i in generated_ids])\n",
    "    return generated_text\n",
    "\n",
    "# Beispiel-Inferenz:\n",
    "# Angenommen, \"mailassist\" trainierte Modelle nutzen den E-Mail-Body,\n",
    "# hier ein Beispiel-Input (unvollständiger E-Mail-Text):\n",
    "example_input = \"sehr geehrte damen und herren, ich möchte sie darüber informieren\"\n",
    "# Tokenisierung: Hier einfach per split (in der Praxis dieselbe Tokenisierung wie in der Vorverarbeitung)\n",
    "example_tokens = example_input.split()\n",
    "input_ids = [word2id.get(token, word2id.get(\"<UNK>\", 1)) for token in example_tokens]\n",
    "\n",
    "completion = generate_completion(model, input_ids, word2id, id2word, max_length=20, device=DEVICE)\n",
    "print(\"\\nBeispiel für E-Mail-Vervollständigung:\")\n",
    "print(\"Input:\", example_input)\n",
    "print(\"Vervollständigung:\", completion)\n",
    "\n",
    "# Zelle 8: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Fazit:\n",
    "- Das Seq2Seq-Modell für die E-Mail-Vervollständigung wurde erfolgreich trainiert.\n",
    "- Die Evaluierung zeigt einen abnehmenden Validierungs-Loss.\n",
    "- Erste Inferenz-Demos liefern sinnvolle Vorschläge für E-Mail-Vervollständigungen.\n",
    "Nächste Schritte:\n",
    "- Training auf einem größeren Datensatz.\n",
    "- Experimentiere mit verschiedenen Hyperparametern und Sampling-Methoden.\n",
    "- Optional: Erweiterung um Transformer-Modelle für noch natürlichere Textvorschläge.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
