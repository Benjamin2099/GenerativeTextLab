{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# 2_data_preprocessing.ipynb\n",
    "# Bereinigung, Tokenisierung & Splitting für MailAssist\n",
    "# ==============================================\n",
    "\n",
    "# Zelle 1: Importiere benötigte Bibliotheken\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Stelle sicher, dass NLTK-Daten vorhanden sind (einmalig ausführen)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Definiere Pfade zu den Rohdaten und zum Speicherort der verarbeiteten Daten\n",
    "RAW_DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Zelle 2: Rohdaten laden\n",
    "# Beispiel: Die CSV-Datei \"emails_raw.csv\" enthält die Spalten \"subject\" und \"body\"\n",
    "raw_file_path = os.path.join(RAW_DATA_DIR, \"emails_raw.csv\")\n",
    "df = pd.read_csv(raw_file_path)\n",
    "print(f\"Anzahl E-Mails: {len(df)}\")\n",
    "display(df.head())\n",
    "\n",
    "# Zelle 3: Datenbereinigung\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Entfernt HTML-Tags, überflüssige Leerzeichen und unerwünschte Zeichen.\n",
    "    Konvertiert den Text in Kleinbuchstaben.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # Entferne HTML-Tags\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # Mehrfache Leerzeichen reduzieren\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Wende die Bereinigung auf Betreff und Body an\n",
    "df[\"clean_subject\"] = df[\"subject\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "# Optional: Entferne E-Mails, deren Body zu kurz sind (z.B. < 20 Zeichen)\n",
    "df = df[df[\"clean_body\"].str.len() > 20]\n",
    "\n",
    "print(\"\\nBeispiel bereinigter E-Mails:\")\n",
    "display(df[[\"clean_subject\", \"clean_body\"]].head())\n",
    "\n",
    "# Zelle 4: Tokenisierung\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Zerlegt den Text in Tokens mittels NLTK.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenisiere Betreff und Body\n",
    "df[\"subject_tokens\"] = df[\"clean_subject\"].apply(tokenize_text)\n",
    "df[\"body_tokens\"] = df[\"clean_body\"].apply(tokenize_text)\n",
    "\n",
    "print(\"\\nBeispiel tokenisierter E-Mails:\")\n",
    "display(df[[\"subject_tokens\", \"body_tokens\"]].head())\n",
    "\n",
    "# Zelle 5: Vokabular erstellen\n",
    "from collections import Counter\n",
    "\n",
    "# Kombiniere alle Tokens aus Betreff und Body\n",
    "all_tokens = []\n",
    "for tokens in df[\"subject_tokens\"]:\n",
    "    all_tokens.extend(tokens)\n",
    "for tokens in df[\"body_tokens\"]:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freqs = Counter(all_tokens)\n",
    "print(f\"Anzahl eindeutiger Tokens: {len(token_freqs)}\")\n",
    "\n",
    "# Beschränke das Vokabular auf die häufigsten Tokens (optional)\n",
    "VOCAB_SIZE = 10000  # Du kannst diesen Wert anpassen\n",
    "most_common_tokens = token_freqs.most_common(VOCAB_SIZE)\n",
    "\n",
    "# Definiere Sondertokens\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for token in special_tokens:\n",
    "    word2id[token] = idx\n",
    "    idx += 1\n",
    "\n",
    "for token, _ in most_common_tokens:\n",
    "    if token not in word2id:\n",
    "        word2id[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "final_vocab_size = len(word2id)\n",
    "print(f\"Endgültige Vokabulargröße (inkl. Sondertokens): {final_vocab_size}\")\n",
    "\n",
    "# Erstelle das inverse Mapping\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# Zelle 6: Tokens in IDs umwandeln\n",
    "def tokens_to_ids(token_list, word2id, unk_id=word2id[\"<UNK>\"]):\n",
    "    \"\"\"\n",
    "    Wandelt eine Liste von Tokens in ihre entsprechenden IDs um.\n",
    "    \"\"\"\n",
    "    return [word2id[t] if t in word2id else unk_id for t in token_list]\n",
    "\n",
    "df[\"subject_ids\"] = df[\"subject_tokens\"].apply(lambda tokens: tokens_to_ids(tokens, word2id))\n",
    "df[\"body_ids\"] = df[\"body_tokens\"].apply(lambda tokens: tokens_to_ids(tokens, word2id))\n",
    "\n",
    "# Füge BOS und EOS zu den E-Mail-Texten hinzu\n",
    "BOS = word2id[\"<BOS>\"]\n",
    "EOS = word2id[\"<EOS>\"]\n",
    "df[\"subject_ids\"] = df[\"subject_ids\"].apply(lambda ids: [BOS] + ids + [EOS])\n",
    "df[\"body_ids\"] = df[\"body_ids\"].apply(lambda ids: [BOS] + ids + [EOS])\n",
    "\n",
    "print(\"\\nBeispiel für Token-IDs:\")\n",
    "display(df[[\"subject_ids\", \"body_ids\"]].head())\n",
    "\n",
    "# Zelle 7: Aufteilung in Trainings-, Validierungs- und Test-Datensätze\n",
    "# Hier nehmen wir an, dass wir sowohl für den Betreff als auch den Body den gleichen Split verwenden.\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Zelle 8: Speichern der verarbeiteten Daten und des Vokabulars\n",
    "import json\n",
    "\n",
    "# Speichere das Vokabular als JSON\n",
    "vocab_path = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2id, f, ensure_ascii=False)\n",
    "print(\"Vokabular gespeichert unter:\", vocab_path)\n",
    "\n",
    "# Speichere die DataFrames als CSV-Dateien\n",
    "train_csv_path = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_csv_path = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "test_csv_path = os.path.join(PROCESSED_DATA_DIR, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)\n",
    "\n",
    "print(\"\\nDaten gespeichert:\")\n",
    "print(\"Train CSV:\", train_csv_path)\n",
    "print(\"Val CSV:\", val_csv_path)\n",
    "print(\"Test CSV:\", test_csv_path)\n",
    "\n",
    "# Zelle 9: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Die E-Mail-Daten wurden erfolgreich bereinigt, tokenisiert und in Trainings-, Validierungs- \n",
    "und Test-Datensätze aufgeteilt. Das Vokabular wurde erstellt und als JSON-Datei gespeichert.\n",
    "Im nächsten Schritt (z. B. in notebooks/3_training_demo.ipynb) wird das MailAssist-Modell \n",
    "trainiert, um automatisierte E-Mail-/Textvorschläge zu generieren.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
