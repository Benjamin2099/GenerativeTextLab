{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken & Setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Verwende Gerät:\", DEVICE)\n",
    "\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 2: Daten laden\n",
    "# ===========================================\n",
    "train_path = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_path = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "test_path = os.path.join(PROCESSED_DATA_DIR, \"test.csv\")\n",
    "\n",
    "vocab_path = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "print(f\"Train Samples: {len(train_df)}, Val Samples: {len(val_df)}, Test Samples: {len(test_df)}\")\n",
    "vocab_size = len(word2id)\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 3: Datensatz & DataLoader erstellen\n",
    "# ===========================================\n",
    "# Wir definieren eine einfache Dataset-Klasse, um\n",
    "# aus den CSV-Spalten \"token_ids\" Tensoren zu erzeugen.\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, seq_col=\"token_ids\"):\n",
    "        # Wir gehen davon aus, dass df[seq_col] eine Liste von Token-IDs in String-Form enthält\n",
    "        # (z.B. \"[1, 45, 23, 99, ...]\")\n",
    "        self.samples = df[seq_col].apply(lambda x: eval(x)).tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        token_ids = self.samples[idx]\n",
    "        # Beispiel: Vorbereiten einer Eingabesequenz (ohne letztes Token)\n",
    "        # und Zielsequenz (ohne erstes Token)\n",
    "        # -> Language-Model-Ansatz \"Next Token Prediction\"\n",
    "        input_seq = token_ids[:-1]\n",
    "        target_seq = token_ids[1:]\n",
    "        \n",
    "        # Tensoren erstellen\n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
    "        target_seq = torch.tensor(target_seq, dtype=torch.long)\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Dataset-Instanzen\n",
    "train_dataset = TextDataset(train_df)\n",
    "val_dataset   = TextDataset(val_df)\n",
    "\n",
    "# Um Batches zu erstellen, brauchen wir einen Collate-Funktion:\n",
    "# -> wir füllen sequences (versch. Längen) auf eine einheitliche Länge per Padding.\n",
    "#   Hier sehr einfach gehalten (Padding auf max. Sequenzlänge im Batch).\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    # batch ist eine Liste von (input_seq, target_seq)\n",
    "    input_seqs, target_seqs = zip(*batch)\n",
    "    \n",
    "    # Pad variable-length sequences\n",
    "    input_padded = pad_sequence(input_seqs, batch_first=True, padding_value=pad_id)\n",
    "    target_padded = pad_sequence(target_seqs, batch_first=True, padding_value=pad_id)\n",
    "    \n",
    "    return input_padded, target_padded\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "print(\"Beispielhafter Batch (Train):\")\n",
    "sample_input, sample_target = next(iter(train_loader))\n",
    "print(\"Input-Shape:\", sample_input.shape, \"Target-Shape:\", sample_target.shape)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 4: LSTM-Modell definieren\n",
    "# ===========================================\n",
    "class LSTMTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1):\n",
    "        super(LSTMTextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [Batch, SeqLen]\n",
    "        emb = self.embedding(x) # [Batch, SeqLen, embed_dim]\n",
    "        out, hidden = self.lstm(emb, hidden) # out: [Batch, SeqLen, hidden_dim]\n",
    "        logits = self.fc(out)   # [Batch, SeqLen, vocab_size]\n",
    "        return logits, hidden\n",
    "\n",
    "# Modell-Instanz\n",
    "model = LSTMTextModel(vocab_size=vocab_size, embed_dim=128, hidden_dim=256, num_layers=2)\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 5: Trainingssetup (Loss, Optimizer)\n",
    "# ===========================================\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Wir ignorieren <PAD>=0 als Target\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 6: Trainingsloop\n",
    "# ===========================================\n",
    "EPOCHS = 3\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_input, batch_target in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        batch_input = batch_input.to(DEVICE)\n",
    "        batch_target = batch_target.to(DEVICE)\n",
    "        \n",
    "        # Forward\n",
    "        logits, _ = model(batch_input)\n",
    "        # logits: [Batch, SeqLen, vocab_size]\n",
    "        # batch_target: [Batch, SeqLen]\n",
    "        \n",
    "        # Wir reshapen, um mit CrossEntropyLoss klarzukommen\n",
    "        logits = logits.reshape(-1, vocab_size)       # -> [Batch*SeqLen, vocab_size]\n",
    "        batch_target = batch_target.view(-1)          # -> [Batch*SeqLen]\n",
    "        \n",
    "        loss = criterion(logits, batch_target)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target in loader:\n",
    "            batch_input = batch_input.to(DEVICE)\n",
    "            batch_target = batch_target.to(DEVICE)\n",
    "            \n",
    "            logits, _ = model(batch_input)\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            batch_target = batch_target.view(-1)\n",
    "            \n",
    "            loss = criterion(logits, batch_target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 7: Kurzer Inferenz-Test\n",
    "# ===========================================\n",
    "# Wir probieren eine einfache Greedy-Generierung aus,\n",
    "# um zu sehen, wie das Modell sich verhält (Frühstadium).\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "def generate_text(model, start_tokens, max_len=20):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([start_tokens], dtype=torch.long).to(DEVICE)\n",
    "    hidden = None\n",
    "    \n",
    "    generated = start_tokens[:]  # Kopie\n",
    "    for _ in range(max_len):\n",
    "        logits, hidden = model(input_seq, hidden)\n",
    "        # logits: [1, SeqLen, vocab_size]\n",
    "        last_logits = logits[0, -1, :]  # letztes Token\n",
    "        next_id = torch.argmax(last_logits).item()  # Greedy\n",
    "        generated.append(next_id)\n",
    "        \n",
    "        input_seq = torch.tensor([generated], dtype=torch.long).to(DEVICE)\n",
    "        \n",
    "        # Optional: Abbruchkriterium (z.B. <EOS>)\n",
    "        if next_id == word2id.get(\"<EOS>\", -1):\n",
    "            break\n",
    "    return generated\n",
    "\n",
    "# Beispiel-Starttokens (BOS oder ein Wort)\n",
    "start_prompt = \"<BOS>\"\n",
    "prompt_id = word2id.get(start_prompt, 1)  # falls nicht im Vokabular -> <UNK>\n",
    "gen_ids = generate_text(model, [prompt_id], max_len=10)\n",
    "\n",
    "generated_words = [id2word[i] for i in gen_ids]\n",
    "print(\"Generated Sequence:\", \" \".join(generated_words))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 8: Speichern des Modells\n",
    "# ===========================================\n",
    "# In echten Projekten speichert man das Modell oft in einem separaten Ordner (z. B. 'models/')\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "model_path = \"../models/lstm_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Modell gespeichert unter:\", model_path)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 9: Zusammenfassung & Ausblick\n",
    "# ===========================================\n",
    "print(\"\"\"\n",
    "In diesem Notebook haben wir ein einfaches LSTM-Sprachmodell auf unseren Daten trainiert.\n",
    "Wir sahen erste Trainingsfortschritte anhand des Losses und konnten ein paar Tokens generieren.\n",
    "\n",
    "Nächste Schritte könnten sein:\n",
    "- Training über mehr Epochen\n",
    "- Hyperparameter-Tuning (Embed-Dim, Hidden-Dim, LR etc.)\n",
    "- Fortgeschrittene Generierung (Top-k-Sampling, Temperature)\n",
    "- Umgang mit sehr langen Sequenzen (Truncation, BPTT-Splitting)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
