{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 2_data_preprocessing.ipynb\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLTK-Token-Daten herunterladen, falls noch nicht geschehen\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Wir definieren Pfade zum \"raw\"- und \"processed\"-Ordner\n",
    "RAW_DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 2: Rohdaten laden\n",
    "# ===========================================\n",
    "# Beispiel: Wir haben eine CSV-Datei mit einer Spalte \"text\".\n",
    "# (Pass an, falls du mehrere Dateien einlesen willst)\n",
    "\n",
    "csv_file_path = os.path.join(RAW_DATA_DIR, \"sample_texts.csv\")\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(f\"Anzahl Datensätze: {len(df)}\")\n",
    "print(\"Erste Zeilen:\")\n",
    "display(df.head())\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 3: Datensäuberung\n",
    "# ===========================================\n",
    "# Beispielhafte Säuberung: Entfernen von HTML-Tags, mehrfachen Leerzeichen, etc.\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # HTML-Tags entfernen\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    # Mehrfache Leerzeichen reduzieren\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Trim\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Optional: Entferne sehr kurze Texte oder null-Werte\n",
    "df.dropna(subset=[\"cleaned_text\"], inplace=True)\n",
    "df = df[df[\"cleaned_text\"].str.len() > 0]\n",
    "\n",
    "print(\"\\nBeispiel für gesäuberten Text:\")\n",
    "display(df[\"cleaned_text\"].head(5))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 4: Tokenisierung\n",
    "# ===========================================\n",
    "# Wir verwenden NLTK's word_tokenize zum Zerlegen in Wort-Token.\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # Optional: Kleinbuchstaben\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "df[\"tokens\"] = df[\"cleaned_text\"].apply(tokenize_text)\n",
    "\n",
    "print(\"\\nBeispiel für Tokens:\")\n",
    "display(df[\"tokens\"].head(5))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 5: Vokabular erstellen\n",
    "# ===========================================\n",
    "# Wir bauen ein Frequenz-Dict, um die häufigsten Tokens zu ermitteln.\n",
    "# Dann werden Tokens in IDs gemappt.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for token_list in df[\"tokens\"]:\n",
    "    all_tokens.extend(token_list)\n",
    "\n",
    "token_freqs = Counter(all_tokens)\n",
    "print(f\"Anzahl eindeutiger Tokens: {len(token_freqs)}\")\n",
    "\n",
    "# Beispiel: Wir beschränken das Vokabular auf die top-N häufigsten Tokens\n",
    "# (falls du sehr große Daten hast)\n",
    "VOCAB_SIZE = 20000  # oder len(token_freqs), wenn du alles behalten willst\n",
    "most_common_tokens = token_freqs.most_common(VOCAB_SIZE)\n",
    "\n",
    "# Wir definieren eine Start-ID ab 4, weil wir 0,1,2,3 z.B. für <PAD>, <UNK>, <BOS>, <EOS> reservieren können\n",
    "word2id = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1,\n",
    "    \"<BOS>\": 2,\n",
    "    \"<EOS>\": 3\n",
    "}\n",
    "idx = 4\n",
    "\n",
    "for token, freq in most_common_tokens:\n",
    "    if token not in word2id:\n",
    "        word2id[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "print(f\"Vokabulargröße (inkl. Sondertokens): {len(word2id)}\")\n",
    "\n",
    "# Umgekehrtes Mapping (ID->Wort)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 6: Token-IDs generieren\n",
    "# ===========================================\n",
    "# Wir ersetzen in jedem Tokensatz die Wörter durch ihre IDs\n",
    "# Tokens, die nicht im Vokabular sind, bekommen <UNK>-ID\n",
    "\n",
    "UNK_ID = word2id[\"<UNK>\"]\n",
    "BOS_ID = word2id[\"<BOS>\"]\n",
    "EOS_ID = word2id[\"<EOS>\"]\n",
    "\n",
    "def tokens_to_ids(token_list, word2id, unk_id=UNK_ID):\n",
    "    return [word2id[t] if t in word2id else unk_id for t in token_list]\n",
    "\n",
    "df[\"token_ids\"] = df[\"tokens\"].apply(lambda tk: tokens_to_ids(tk, word2id))\n",
    "\n",
    "print(\"\\nBeispiel für token_ids:\")\n",
    "display(df[\"token_ids\"].head(5))\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 7: Erstellung von Sequenzen (Train/Val/Test-Splits)\n",
    "# ===========================================\n",
    "# Wir gehen davon aus, dass wir später in src/dataset.py Sequenzen bilden.\n",
    "# Hier zeigen wir exemplarisch, wie du train/val/test trennst.\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 8: Speichern der Ergebnisse\n",
    "# ===========================================\n",
    "# Jetzt speichern wir unser Vokabular und die Datenframes mit token_ids in \"processed\".\n",
    "\n",
    "# 1) Vokabular\n",
    "vocab_path = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2id, f, ensure_ascii=False)\n",
    "\n",
    "# 2) Train/Val/Test CSV (oder Parquet, JSON, etc.)\n",
    "train_path = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "val_path = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "test_path = os.path.join(PROCESSED_DATA_DIR, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"\\nDaten gespeichert in:\")\n",
    "print(train_path)\n",
    "print(val_path)\n",
    "print(test_path)\n",
    "print(\"Vokabular gespeichert in:\", vocab_path)\n",
    "\n",
    "# ===========================================\n",
    "# Zelle 9: Ausblick\n",
    "# ===========================================\n",
    "# An dieser Stelle haben wir:\n",
    "#  - Die Rohdaten bereinigt\n",
    "#  - Tokens erstellt\n",
    "#  - Ein Vokabular generiert & ID-Mappings\n",
    "#  - Einfache Train/Val/Test-Splits angelegt\n",
    "#  - Alles in data/processed/ gespeichert\n",
    "\n",
    "print(\"\\nNächster Schritt: In '3_training_demo.ipynb' oder in src/train.py können wir nun das LSTM-Modell trainieren.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
