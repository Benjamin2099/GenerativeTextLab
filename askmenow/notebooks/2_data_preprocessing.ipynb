{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 2_data_preprocessing.ipynb\n",
    "# Bereinigung, Tokenisierung & Vektorisierung (Embeddings) für AskMeNow\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken importieren\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stelle sicher, dass NLTK-Daten verfügbar sind\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline\n",
    "\n",
    "# Zelle 2: Pfade definieren und Rohdaten laden\n",
    "RAW_DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "EMBEDDING_DIR = os.path.join(\"..\", \"data\", \"embeddings\")\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "\n",
    "# Angenommen, die FAQ-Daten liegen in einer CSV-Datei \"faq_data.csv\" mit den Spalten \"question\" und \"answer\"\n",
    "data_file = os.path.join(RAW_DATA_DIR, \"faq_data.csv\")\n",
    "df = pd.read_csv(data_file)\n",
    "print(\"Anzahl der FAQ-Einträge:\", len(df))\n",
    "display(df.head())\n",
    "\n",
    "# Zelle 3: Datenbereinigung\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Entfernt HTML-Tags, überflüssige Leerzeichen und konvertiert den Text in Kleinbuchstaben.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_answer\"] = df[\"answer\"].apply(clean_text)\n",
    "print(\"\\nBereinigte Fragen und Antworten:\")\n",
    "display(df[[\"clean_question\", \"clean_answer\"]].head())\n",
    "\n",
    "# Zelle 4: Optional: Tokenisierung (für weitere Analysen)\n",
    "df[\"question_tokens\"] = df[\"clean_question\"].apply(word_tokenize)\n",
    "df[\"answer_tokens\"] = df[\"clean_answer\"].apply(word_tokenize)\n",
    "print(\"\\nBeispiel tokenisierter Texte:\")\n",
    "display(df[[\"question_tokens\", \"answer_tokens\"]].head())\n",
    "\n",
    "# Zelle 5: Vektorisierung mit einem vortrainierten Transformer-Modell\n",
    "# Wir nutzen z. B. \"sentence-transformers/all-MiniLM-L6-v2\" für die Berechnung von Satz-Embeddings.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Verwende Gerät:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_sentence_embedding(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Berechnet das Embedding eines Satzes mittels Mean-Pooling der Token-Embeddings.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    return mean_pooled.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Berechne Embeddings für alle Fragen und Antworten\n",
    "question_embeddings = []\n",
    "answer_embeddings = []\n",
    "\n",
    "print(\"\\nBerechne Embeddings für Fragen:\")\n",
    "for text in tqdm(df[\"clean_question\"], desc=\"Fragen Embeddings\"):\n",
    "    emb = get_sentence_embedding(text, tokenizer, model, device)\n",
    "    question_embeddings.append(emb)\n",
    "\n",
    "print(\"\\nBerechne Embeddings für Antworten:\")\n",
    "for text in tqdm(df[\"clean_answer\"], desc=\"Antworten Embeddings\"):\n",
    "    emb = get_sentence_embedding(text, tokenizer, model, device)\n",
    "    answer_embeddings.append(emb)\n",
    "\n",
    "# Zelle 6: Speichern der Embeddings und der verarbeiteten Daten\n",
    "# Speichere die Embeddings als NumPy-Arrays\n",
    "np.save(os.path.join(EMBEDDING_DIR, \"question_embeddings.npy\"), np.array(question_embeddings))\n",
    "np.save(os.path.join(EMBEDDING_DIR, \"answer_embeddings.npy\"), np.array(answer_embeddings))\n",
    "print(\"\\nEmbeddings wurden im Ordner\", EMBEDDING_DIR, \"gespeichert.\")\n",
    "\n",
    "# Speichere auch den bereinigten DataFrame als CSV\n",
    "processed_csv_path = os.path.join(PROCESSED_DATA_DIR, \"faq_processed.csv\")\n",
    "df.to_csv(processed_csv_path, index=False)\n",
    "print(\"Verarbeitete FAQ-Daten gespeichert unter:\", processed_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
