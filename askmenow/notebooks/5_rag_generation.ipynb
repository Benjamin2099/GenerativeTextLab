{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 5_rag_generation.ipynb\n",
    "# Kombination von Retrieval (BM25) und Transformer-Generierung (RAG)\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken importieren\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten vorhanden sind\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline\n",
    "\n",
    "# Zelle 2: Dummy-Wissensbasis laden (in der Praxis aus einer CSV)\n",
    "# Hier simulieren wir eine kleine Wissensbasis als Liste von Textpassagen.\n",
    "knowledge_documents = [\n",
    "    \"Neuronale Netze sind Algorithmen, die lose vom menschlichen Gehirn inspiriert sind und zur Mustererkennung eingesetzt werden.\",\n",
    "    \"Machine Learning ist ein Teilbereich der künstlichen Intelligenz, der es Computern ermöglicht, aus Daten zu lernen.\",\n",
    "    \"Deep Learning verwendet tiefe neuronale Netzwerke, um komplexe Muster und Strukturen in großen Datensätzen zu erkennen.\",\n",
    "    \"Die Relativitätstheorie wurde von Albert Einstein entwickelt und beschreibt die Gravitation als Folge der Krümmung von Raum und Zeit.\",\n",
    "    \"Künstliche Intelligenz wird in vielen Bereichen eingesetzt, von der Medizin bis zur autonomen Fahrzeugsteuerung.\"\n",
    "]\n",
    "\n",
    "# Zelle 3: BM25-Retriever aufbauen\n",
    "# Tokenisiere alle Dokumente\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in knowledge_documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Zelle 4: Eingabe-Frage und Retrieval\n",
    "input_question = \"Wie funktionieren neuronale Netze?\"\n",
    "tokenized_question = word_tokenize(input_question.lower())\n",
    "\n",
    "# Abrufen der Top 3 relevanten Dokumente\n",
    "top_k = 3\n",
    "retrieved_indices = bm25.get_top_n(tokenized_question, knowledge_documents, n=top_k)\n",
    "print(\"Top abgerufene Dokumente:\")\n",
    "for doc in retrieved_indices:\n",
    "    print(\"- \", doc)\n",
    "\n",
    "# Zelle 5: Prompt für den Transformer generieren\n",
    "# Kombiniere die abgerufenen Dokumente in einem Prompt\n",
    "prompt = \"Frage: \" + input_question + \"\\nWissensbasis:\\n\"\n",
    "for idx, doc in enumerate(retrieved_indices, 1):\n",
    "    prompt += f\"{idx}. {doc}\\n\"\n",
    "prompt += \"Antwort:\"\n",
    "\n",
    "print(\"\\nGenerierungsprompt:\\n\", prompt)\n",
    "\n",
    "# Zelle 6: Transformer (T5) zur Antwortgenerierung\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "t5_model.to(device)\n",
    "t5_model.eval()\n",
    "\n",
    "# Tokenisiere den Prompt\n",
    "inputs = t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generiere Antwort mit Beam Search\n",
    "output_ids = t5_model.generate(\n",
    "    **inputs,\n",
    "    num_beams=4,\n",
    "    max_length=150,\n",
    "    early_stopping=True\n",
    ")\n",
    "generated_answer = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerierte Antwort:\")\n",
    "print(generated_answer)\n",
    "\n",
    "# Zelle 7: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Fazit:\n",
    "- Der BM25-Retriever extrahiert relevante Textpassagen aus der Wissensbasis basierend auf der Eingabefrage.\n",
    "- Die abgerufenen Dokumente werden in einen Prompt integriert, der an ein vortrainiertes T5-Modell übergeben wird.\n",
    "- Das T5-Modell generiert auf Basis dieses kombinierten Prompts eine faktenbasierte Antwort.\n",
    "Nächste Schritte:\n",
    "- Integration einer größeren Wissensdatenbank (z. B. mit FAISS für schnelle Vektor-Suche).\n",
    "- Feinabstimmung des Transformer-Modells (z. B. mit RLHF), um die Antwortqualität weiter zu verbessern.\n",
    "- Evaluation der Antworten mittels geeigneter Metriken.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
