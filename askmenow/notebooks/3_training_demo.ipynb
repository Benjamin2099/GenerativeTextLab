{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# Training von Q&A-Modellen (Reader) für AskMeNow\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Importiere benötigte Bibliotheken und Setup\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gerät festlegen: GPU falls verfügbar, sonst CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Trainiere auf Gerät:\", DEVICE)\n",
    "\n",
    "# Zelle 2: Definiere Pfade und lade Daten/Vokabular\n",
    "PROCESSED_DATA_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "TRAIN_CSV = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")  # Erwartet Spalten \"question_ids\" und \"answer_ids\"\n",
    "VAL_CSV   = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "VOCAB_JSON = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "# Lade Trainings- und Validierungsdaten\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_val = pd.read_csv(VAL_CSV)\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "pad_id = word2id.get(\"<PAD>\", 0)\n",
    "print(f\"Train Samples: {len(df_train)} | Val Samples: {len(df_val)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "display(df_train.head(3))\n",
    "\n",
    "# Zelle 3: Dataset-Klasse für Q&A (Fragen und Antworten)\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset für Q&A-Aufgaben.\n",
    "    Erwartet eine CSV-Datei mit den Spalten:\n",
    "      - \"question_ids\": Tokenisierte Frage (als Liste, z. B. \"[2, 45, 78, 3]\")\n",
    "      - \"answer_ids\": Tokenisierte Antwort (als Liste, z. B. \"[2, 56, 90, 3]\")\n",
    "    \"\"\"\n",
    "    def __init__(self, df, question_col=\"question_ids\", answer_col=\"answer_ids\"):\n",
    "        self.df = df.copy()\n",
    "        # Konvertiere String-Repräsentationen in echte Listen\n",
    "        self.df[question_col] = self.df[question_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.df[answer_col] = self.df[answer_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.questions = self.df[question_col].tolist()\n",
    "        self.answers = self.df[answer_col].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question_ids = self.questions[idx]\n",
    "        answer_ids = self.answers[idx]\n",
    "        question_tensor = torch.tensor(question_ids, dtype=torch.long)\n",
    "        answer_tensor = torch.tensor(answer_ids, dtype=torch.long)\n",
    "        return question_tensor, answer_tensor\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    questions, answers = zip(*batch)\n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_id)\n",
    "    answers_padded = pad_sequence(answers, batch_first=True, padding_value=pad_id)\n",
    "    return questions_padded, answers_padded\n",
    "\n",
    "def create_qa_dataloader(csv_file, batch_size=32, shuffle=True, pad_id=0, question_col=\"question_ids\", answer_col=\"answer_ids\"):\n",
    "    dataset = QADataset(pd.read_csv(csv_file), question_col=question_col, answer_col=answer_col)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id)\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "train_loader = create_qa_dataloader(TRAIN_CSV, batch_size=32, shuffle=True, pad_id=pad_id)\n",
    "val_loader = create_qa_dataloader(VAL_CSV, batch_size=32, shuffle=False, pad_id=pad_id)\n",
    "\n",
    "print(\"Beispielhafter Batch:\")\n",
    "q_batch, a_batch = next(iter(train_loader))\n",
    "print(\"Questions-Shape:\", q_batch.shape, \"Answers-Shape:\", a_batch.shape)\n",
    "\n",
    "# Zelle 4: Modell definieren (Seq2Seq-LSTM für Q&A)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # [Batch, src_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded)  # [Batch, src_len, hidden_dim]\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, trg, hidden):\n",
    "        embedded = self.embedding(trg)  # [Batch, trg_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded, hidden)  # [Batch, trg_len, hidden_dim]\n",
    "        predictions = self.fc(outputs)  # [Batch, trg_len, vocab_size]\n",
    "        return predictions, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Erster Decoder-Input: <BOS>-Token (erster Token in trg)\n",
    "        input_dec = trg[:, 0].unsqueeze(1)  # [Batch, 1]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_dec, hidden)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(dim=2)  # [Batch, 1]\n",
    "            input_dec = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "encoder_model = Encoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "decoder_model = Decoder(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, pad_idx=pad_id)\n",
    "model = Seq2Seq(encoder_model, decoder_model, pad_id).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Zelle 5: Trainings-Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for questions, answers in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        questions, answers = questions.to(device), answers.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(questions, answers, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        output_dim = outputs.shape[-1]\n",
    "        outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "        targets = answers[:, 1:].reshape(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for questions, answers in loader:\n",
    "            questions, answers = questions.to(device), answers.to(device)\n",
    "            outputs = model(questions, answers, teacher_forcing_ratio=0.0)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].reshape(-1, output_dim)\n",
    "            targets = answers[:, 1:].reshape(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Zelle 6: Trainings- und Validierungsschleife\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoche {epoch}/{EPOCHS} ===\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, teacher_forcing_ratio=0.5)\n",
    "    val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = os.path.join(\"models\", f\"askmenow_epoch{epoch}_valloss{val_loss:.4f}.pt\")\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"** Neues bestes Modell gespeichert: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining abgeschlossen.\")\n",
    "\n",
    "# Zelle 7: Inferenz-Demo – Generierung einer Antwort\n",
    "def generate_answer(model, question_ids, word2id, id2word, max_length=20, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    # Konvertiere die Frage in einen Tensor\n",
    "    src_tensor = torch.tensor([question_ids], dtype=torch.long).to(device)\n",
    "    _, hidden = encoder_model(src_tensor)\n",
    "    # Initialer Decoder-Input: <BOS>-Token\n",
    "    bos_id = word2id.get(\"<BOS>\", 2)\n",
    "    input_dec = torch.tensor([[bos_id]], dtype=torch.long).to(device)\n",
    "    generated_ids = [bos_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = decoder_model(input_dec, hidden)\n",
    "            next_id = output.argmax(dim=2).item()\n",
    "            if next_id == word2id.get(\"<EOS>\", 3):\n",
    "                break\n",
    "            generated_ids.append(next_id)\n",
    "            input_dec = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    generated_answer = \" \".join([id2word.get(i, \"<UNK>\") for i in generated_ids])\n",
    "    return generated_answer\n",
    "\n",
    "# Beispiel-Inferenz:\n",
    "example_question = \"Wie funktioniert ein neuronales Netz?\"\n",
    "# Tokenisierung: einfache Splittung (in der Praxis dieselbe Logik wie in der Vorverarbeitung)\n",
    "example_tokens = example_question.strip().lower().split()\n",
    "question_ids = [word2id.get(token, word2id.get(\"<UNK>\", 1)) for token in example_tokens]\n",
    "\n",
    "generated_answer = generate_answer(model, question_ids, word2id, id2word, max_length=20, device=DEVICE)\n",
    "print(\"\\nBeispiel generierte Antwort:\")\n",
    "print(\"Frage:\", example_question)\n",
    "print(\"Antwort:\", generated_answer)\n",
    "\n",
    "# Zelle 8: Fazit & Ausblick\n",
    "print(\"\"\"\n",
    "Fazit:\n",
    "- Das Q&A-Modell (Reader) wurde erfolgreich auf den FAQ-Daten trainiert.\n",
    "- Die Evaluierung zeigt einen kontinuierlich sinkenden Validierungs-Loss.\n",
    "- Erste Inferenz-Demos liefern sinnvolle Antworten auf gestellte Fragen.\n",
    "Nächste Schritte:\n",
    "- Training auf größeren und diversifizierten Datensätzen.\n",
    "- Erweiterung des Modells um Transformer-basierte Ansätze.\n",
    "- Integration von Retrieval-Komponenten zur Verbesserung der Antwortgenauigkeit.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
