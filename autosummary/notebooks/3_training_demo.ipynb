{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 3_training_demo.ipynb\n",
    "# Prototypisches Training & erste Inferenz-Demos für AutoSummary\n",
    "# ===========================================\n",
    "\n",
    "# Zelle 1: Bibliotheken und Setup\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Verwende Gerät:\", DEVICE)\n",
    "\n",
    "# Verzeichnispfade\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAIN_CSV = os.path.join(PROCESSED_DATA_DIR, \"train.csv\")\n",
    "VAL_CSV   = os.path.join(PROCESSED_DATA_DIR, \"val.csv\")\n",
    "VOCAB_JSON = os.path.join(PROCESSED_DATA_DIR, \"vocab.json\")\n",
    "\n",
    "# Zelle 2: Daten laden\n",
    "# Annahme: Die CSV-Dateien enthalten zwei Spalten:\n",
    "# \"article_ids\" (Liste von Token-IDs des Originaltextes)\n",
    "# \"summary_ids\" (Liste von Token-IDs der Zusammenfassung)\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_val   = pd.read_csv(VAL_CSV)\n",
    "\n",
    "with open(VOCAB_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2id = json.load(f)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "pad_id = word2id.get(\"<PAD>\", 0)\n",
    "\n",
    "print(f\"Train Samples: {len(df_train)}, Val Samples: {len(df_val)}\")\n",
    "print(\"Vokabulargröße:\", vocab_size)\n",
    "display(df_train.head(2))\n",
    "\n",
    "# Zelle 3: Dataset und DataLoader\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset für AutoSummary.\n",
    "    Erwartet eine CSV mit den Spalten \"article_ids\" und \"summary_ids\".\n",
    "    Die Spalten enthalten tokenisierte Listen (als String, z.B. \"[2, 45, 78, 3]\").\n",
    "    \"\"\"\n",
    "    def __init__(self, df, article_col=\"article_ids\", summary_col=\"summary_ids\"):\n",
    "        self.df = df.copy()\n",
    "        # Konvertiere die Stringrepräsentation in echte Listen\n",
    "        self.df[article_col] = self.df[article_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.df[summary_col] = self.df[summary_col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        self.articles = self.df[article_col].tolist()\n",
    "        self.summaries = self.df[summary_col].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article_ids = self.articles[idx]\n",
    "        summary_ids = self.summaries[idx]\n",
    "        article_tensor = torch.tensor(article_ids, dtype=torch.long)\n",
    "        summary_tensor = torch.tensor(summary_ids, dtype=torch.long)\n",
    "        return article_tensor, summary_tensor\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    articles, summaries = zip(*batch)\n",
    "    articles_padded = pad_sequence(articles, batch_first=True, padding_value=pad_id)\n",
    "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value=pad_id)\n",
    "    return articles_padded, summaries_padded\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = SummarizationDataset(df_train)\n",
    "val_dataset   = SummarizationDataset(df_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "\n",
    "# Zelle 4: Seq2Seq-Modell (Encoder-Decoder) definieren\n",
    "\n",
    "# Encoder: Liest den Input-Text (Artikel)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src: [Batch, src_len]\n",
    "        embedded = self.embedding(src)  # [Batch, src_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded)  # outputs: [Batch, src_len, hidden_dim]\n",
    "        return outputs, hidden\n",
    "\n",
    "# Decoder: Generiert die Zusammenfassung Token für Token\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, trg, hidden):\n",
    "        # trg: [Batch, trg_len]\n",
    "        embedded = self.embedding(trg)  # [Batch, trg_len, embed_dim]\n",
    "        outputs, hidden = self.lstm(embedded, hidden)  # [Batch, trg_len, hidden_dim]\n",
    "        predictions = self.fc(outputs)  # [Batch, trg_len, vocab_size]\n",
    "        return predictions, hidden\n",
    "\n",
    "# Seq2Seq-Modell: Kombiniert Encoder und Decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [Batch, src_len] (Artikel)\n",
    "            trg: [Batch, trg_len] (Zusammenfassung, inklusive <BOS> am Anfang)\n",
    "            teacher_forcing_ratio: Wahrscheinlichkeit, dass das Modell das wahre Token als Input nutzt.\n",
    "        Returns:\n",
    "            outputs: [Batch, trg_len, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        # Ausgabe-Tensor initialisieren\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        \n",
    "        # Encoder: Lies den Artikel\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # Initialer Input für den Decoder: das erste Token der Zusammenfassung (<BOS>)\n",
    "        input_dec = trg[:, 0].unsqueeze(1)  # [Batch, 1]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_dec, hidden)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            \n",
    "            # Entscheide, ob Teacher Forcing genutzt wird\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)  # [Batch, 1]\n",
    "            input_dec = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialisierung des Modells\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "encoder = Encoder(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, pad_id)\n",
    "decoder = Decoder(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, pad_id)\n",
    "model = Seq2Seq(encoder, decoder, pad_id).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Zelle 5: Trainings-Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 3\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, trg in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward-Pass: trg enthält den gesamten Zieltext (mit <BOS> am Anfang)\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # Output: [Batch, trg_len, vocab_size] -> reshape für Loss: [Batch*trg_len, vocab_size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  # Ignoriere das erste Token (<BOS>)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Zelle 6: Trainingsschleife\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\n=== Epoche {epoch}/{EPOCHS} ===\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, teacher_forcing_ratio=0.5)\n",
    "    val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Speichern des besten Modells (Pfad anpassen)\n",
    "        torch.save(model.state_dict(), f\"autosummary_best_epoch{epoch}.pt\")\n",
    "        print(\"** Bestes Modell gespeichert.\")\n",
    "\n",
    "# Zelle 7: Inferenz-Demo – Zusammenfassung generieren\n",
    "def generate_summary(model, article_ids, word2id, id2word, max_length=50):\n",
    "    \"\"\"\n",
    "    Generiert eine Zusammenfassung für einen gegebenen Artikel (Liste von Token-IDs).\n",
    "    Die Funktion nutzt Greedy-Decoding.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # article_ids: Liste von Token-IDs des Artikels\n",
    "    src = torch.tensor([article_ids], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    # Encoder: Artikel verarbeiten\n",
    "    encoder_outputs, hidden = encoder(src)\n",
    "    \n",
    "    # Decoder-Start: Setze das erste Token auf <BOS> für die Zusammenfassung\n",
    "    bos_id = word2id.get(\"<BOS>\", 2)\n",
    "    input_dec = torch.tensor([[bos_id]], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    generated_ids = [bos_id]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        output, hidden = decoder(input_dec, hidden)\n",
    "        # output: [1, 1, vocab_size]\n",
    "        next_id = output.argmax(2).item()\n",
    "        if next_id == word2id.get(\"<EOS>\", 3):\n",
    "            break\n",
    "        generated_ids.append(next_id)\n",
    "        input_dec = torch.tensor([[next_id]], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    return generated_ids\n",
    "\n",
    "# Beispiel-Inferenz\n",
    "# Wähle einen Artikel aus dem Validierungsdatensatz\n",
    "sample_article_ids = df_val[\"article_ids\"].iloc[0]\n",
    "# Konvertiere den Artikel (String) in eine Liste, falls nötig\n",
    "if isinstance(sample_article_ids, str):\n",
    "    sample_article_ids = eval(sample_article_ids)\n",
    "\n",
    "gen_ids = generate_summary(model, sample_article_ids, word2id, id2word, max_length=50)\n",
    "gen_summary = \" \".join([id2word.get(i, \"<UNK>\") for i in gen_ids])\n",
    "print(\"\\nGenerierte Zusammenfassung:\")\n",
    "print(gen_summary)\n",
    "\n",
    "# Zelle 8: Ausblick & Fazit\n",
    "print(\"\"\"\n",
    "In diesem Notebook haben wir:\n",
    "- Einen Prototypen für ein Seq2Seq-Modell (Encoder-Decoder-LSTM) zum Erzeugen von Zusammenfassungen trainiert.\n",
    "- Den Trainingsprozess und die Evaluierung demonstriert.\n",
    "- Eine erste Inferenz-Demo durchgeführt, bei der ein Artikel zusammengefasst wurde.\n",
    "\n",
    "Nächste Schritte:\n",
    "- Mehr Daten und Epochen zum besseren Training nutzen.\n",
    "- Experimentiere mit Teacher Forcing, Sampling-Methoden und Hyperparametern.\n",
    "- Erweitere den Ansatz um Transformer-Modelle (z.B. T5 oder BART) für qualitativ hochwertigere Zusammenfassungen.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
